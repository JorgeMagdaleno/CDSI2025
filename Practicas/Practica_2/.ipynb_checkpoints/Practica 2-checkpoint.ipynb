{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af78c865-a4b8-4516-b789-b04c59ba1e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install mediapipe opencv-python pandas dtw hmmlearn fastdtw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dac3dc-50b6-4d9e-a8de-f1ee62108ca2",
   "metadata": {},
   "source": [
    "==================== Recuperacion de datos ==================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2457b9bb-208f-4ed2-b7df-48d20a194419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Para hacer formating de los videos originales\n",
    "video_folder = \"Videos\"\n",
    "person_prefix = \"Person\"\n",
    "action_prefix = \"Action\"\n",
    "take_prefix = \"take\"\n",
    "\n",
    "video_files = sorted([f for f in os.listdir(video_folder) if f.startswith(\"DSC_\")])\n",
    "\n",
    "\n",
    "person_count = 1\n",
    "action_count = 1\n",
    "take_count = 1\n",
    "files_per_person = 15\n",
    "files_per_action = 3\n",
    "\n",
    "# Guardarlos con sus nuevos nombres para manejo mas sencillo\n",
    "for i, filename in enumerate(video_files):\n",
    "\n",
    "    new_name = f\"{person_prefix} {person_count} {action_prefix} {action_count} {take_prefix} {take_count}{os.path.splitext(filename)[1]}\"\n",
    "    old_path = os.path.join(video_folder, filename)\n",
    "    new_path = os.path.join(video_folder, new_name)\n",
    "\n",
    "    os.rename(old_path, new_path)\n",
    "    print(f\"Renamed: {filename} -> {new_name}\")\n",
    "\n",
    "    take_count += 1\n",
    "    if take_count > files_per_action:\n",
    "        take_count = 1\n",
    "        action_count += 1\n",
    "\n",
    "    if action_count > 5:\n",
    "        action_count = 1\n",
    "        person_count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a020ef2-a535-41ef-b1b0-c396f930e0d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jorgemagdaleno/PycharmProjects/pythonProject/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1739422966.448556 19529424 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M1 Pro\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Inicializar Mediapipe\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose()\n",
    "\n",
    "# Funcion paratomar todos los videos y sus caracteristicas por frame \n",
    "def get_videos(video_folder):\n",
    "    output = []\n",
    "    video_files = sorted([f for f in os.listdir(video_folder) if f.endswith(('.mp4', '.avi', '.MOV'))])\n",
    "    \n",
    "    for video_file in video_files:\n",
    "        video_path = os.path.join(video_folder, video_file)\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frame_count = 0\n",
    "        video_data = []\n",
    "    \n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "    \n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            results = pose.process(frame_rgb)\n",
    "    \n",
    "            if results.pose_landmarks:\n",
    "                frame_keypoints = {}\n",
    "                for idx, landmark in enumerate(results.pose_landmarks.landmark):\n",
    "                    frame_keypoints[f\"landmark_{idx}\"] = {\n",
    "                        \"x\": landmark.x,\n",
    "                        \"y\": landmark.y,\n",
    "                        \"z\": landmark.z,\n",
    "                        \"visibility\": landmark.visibility\n",
    "                    }\n",
    "                video_data.append({\"frame\": frame_count, \"keypoints\": frame_keypoints})\n",
    "    \n",
    "            frame_count += 1\n",
    "    \n",
    "        cap.release()\n",
    "        output.append({\"video\": video_file, \"frames\": video_data})\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b39a984-c1ab-43db-9f23-ff5f9a0834d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1739422966.505243 19529570 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1739422966.518876 19529569 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1739422966.542566 19529572 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n",
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output_data = get_videos(\"Videos\")\n",
    "print(output_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef81961-e292-4a86-8eca-4ee8a9597cb1",
   "metadata": {},
   "source": [
    "==================== Aumentar Datos y normalizacion ===================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "417fc72f-1363-44c3-8530-eaab5e890783",
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_pose(keypoints):\n",
    "    # Usar los puntos de las caderas para centrar al sujeto\n",
    "    lh = keypoints[\"landmark_23\"]\n",
    "    rh = keypoints[\"landmark_24\"]\n",
    "    \n",
    "    center_x = (lh[\"x\"] + rh[\"x\"]) / 2\n",
    "    center_y = (lh[\"y\"] + rh[\"y\"]) / 2\n",
    "    \n",
    "    centered = {}\n",
    "    for key, coords in keypoints.items():\n",
    "        centered[key] = {\n",
    "            \"x\": coords[\"x\"] - center_x,\n",
    "            \"y\": coords[\"y\"] - center_y,\n",
    "            \"z\": coords[\"z\"],\n",
    "            \"visibility\": coords[\"visibility\"]\n",
    "        }\n",
    "    return centered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80e5dc10-ff36-470e-b573-49ce76152d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def scale_pose(centered_keypoints):\n",
    "    # Calcular que tan lejos esta por los dos puntos de la cadera y escalarlo para tener datos mas similares\n",
    "    lh = centered_keypoints[\"landmark_23\"]\n",
    "    rh = centered_keypoints[\"landmark_24\"]\n",
    "    scale = math.sqrt((lh[\"x\"] - rh[\"x\"])**2 + (lh[\"y\"] - rh[\"y\"])**2)\n",
    "\n",
    "    # En caso de que la escala sea 0 mejor ignorarlo y no escalar\n",
    "    if scale == 0:\n",
    "        scale = 1.0\n",
    "\n",
    "    scaled = {}\n",
    "    for key, coords in centered_keypoints.items():\n",
    "        scaled[key] = {\n",
    "            \"x\": coords[\"x\"] / scale,\n",
    "            \"y\": coords[\"y\"] / scale,\n",
    "            \"z\": coords[\"z\"],\n",
    "            \"visibility\": coords[\"visibility\"]\n",
    "        }\n",
    "    return scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "26407306-d037-4665-9c87-1efde5e14f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_pose(keypoints):\n",
    "    centered = center_pose(keypoints)\n",
    "    normalized = scale_pose(centered)\n",
    "    return normalized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d10391e-10fa-4091-a457-4ed7c8f39c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_normalized_features(keypoints):\n",
    "    normalized = normalize_pose(keypoints)\n",
    "    features = []\n",
    "    for i in range(33):  # Assuming 33 landmarks\n",
    "        lm = normalized.get(f\"landmark_{i}\", {\"x\": 0, \"y\": 0, \"z\": 0, \"visibility\": 0})\n",
    "        features.extend([lm[\"x\"], lm[\"y\"], lm[\"z\"], lm[\"visibility\"]])\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac4957e2-814a-4324-9bb7-5fe1840a50a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_sequence(sequence, noise_std=0.01):\n",
    "    #Esta secuencia de augmentacion realiza solamente la inversion de la imagen y anade un poco de ruido al reflejo apra evitar que sean exactamente iguales\n",
    "    seq_aug = sequence.copy()\n",
    "    num_landmarks = seq_aug.shape[1] // 4\n",
    "    T = seq_aug.shape[0]\n",
    "    \n",
    "    for t in range(T): \n",
    "        for i in range(num_landmarks):\n",
    "            base_idx = i * 4\n",
    "            \n",
    "            original_x = seq_aug[t, base_idx]\n",
    "            seq_aug[t, base_idx] = 1.0 - original_x + np.random.normal(0, noise_std)\n",
    "            \n",
    "            seq_aug[t, base_idx + 1] += np.random.normal(0, noise_std)\n",
    "            seq_aug[t, base_idx + 2] += np.random.normal(0, noise_std)\n",
    "    \n",
    "    return seq_aug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3416f7f5-8b4a-4298-9e43-18b328ef3dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Crear el CSV y aplicar la aumentacion  de datos y normalizacion\n",
    "def data_to_csv(output, name):\n",
    "    csv_data = []\n",
    "    for video in output:\n",
    "        video_name_parts = video[\"video\"].split()\n",
    "        action_num = int(video_name_parts[3])\n",
    "        take_num = int(video_name_parts[5].split(\".\")[0])\n",
    "    \n",
    "        completion_value = {1: 1.0, 2: 0.5, 3: 0.0}.get(take_num, 0.0)\n",
    "    \n",
    "        #Procesamiento de los frames originales\n",
    "        original_sequence_features = []\n",
    "        for frame in video[\"frames\"]:\n",
    "            # Normalizar\n",
    "            normalized_keypoints = normalize_pose(frame[\"keypoints\"])\n",
    "            features = extract_normalized_features(normalized_keypoints)\n",
    "            original_sequence_features.append(features)\n",
    "            \n",
    "            row = {\n",
    "                \"video\": video[\"video\"],\n",
    "                \"frame\": frame[\"frame\"],\n",
    "                \"action\": f\"Action {action_num}\",\n",
    "                \"completion\": completion_value\n",
    "            }\n",
    "            for idx, value in enumerate(features):\n",
    "                row[f\"feat_{idx}\"] = value\n",
    "                \n",
    "            csv_data.append(row)\n",
    "        \n",
    "        sequence_array = np.array(original_sequence_features)\n",
    "        \n",
    "        # Aumentar los datos\n",
    "        augmented_sequence = augment_sequence(sequence_array, noise_std=0.01)\n",
    "\n",
    "        for i, frame in enumerate(video[\"frames\"]):\n",
    "            features = augmented_sequence[i]\n",
    "            row = {\n",
    "                \"video\": video[\"video\"] + \"_aug\",\n",
    "                \"frame\": frame[\"frame\"],\n",
    "                \"action\": f\"Action {action_num}\",\n",
    "                \"completion\": completion_value\n",
    "            }\n",
    "            for idx, value in enumerate(features):\n",
    "                row[f\"feat_{idx}\"] = value\n",
    "            csv_data.append(row)\n",
    "\n",
    "    # Guardarlo en CSV\n",
    "    df = pd.DataFrame(csv_data)\n",
    "    csv_output_path = name\n",
    "    df.to_csv(csv_output_path, index=False)\n",
    "    \n",
    "    print(f\"Pose data saved to {csv_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de9c00fb-7478-40dc-ab2c-e92714466230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pose data saved to pose_data.csv\n"
     ]
    }
   ],
   "source": [
    "data_to_csv(output_data,\"pose_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a97b9d-28ea-4a0c-861d-e9ae65392143",
   "metadata": {},
   "source": [
    "==================== Modelos ===================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a28b1fe-c6ee-4418-8d96-bc8e95b16623",
   "metadata": {},
   "source": [
    "======== Para predecir Accion =========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a98040be-c664-4219-ba68-bedc67910f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Clasificación de Acción con DTW kNN ===\n",
      "Precisión: 0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "# Cargar y preparar datos desde \"pose_data.csv\"\n",
    "df = pd.read_csv(\"pose_data.csv\")\n",
    "# Seleccionar las columnas de caracteristicas que comienzan con \"feat\"\n",
    "feature_cols = [col for col in df.columns if col.startswith(\"feat\")]\n",
    "\n",
    "# Ordenar por video y frame\n",
    "df = df.sort_values(['video', 'frame'])\n",
    "\n",
    "# Agrupar los frames por video (cada video/toma es una secuencia)\n",
    "grouped = df.groupby('video')\n",
    "\n",
    "X_sequences = []\n",
    "y_action_seq = []\n",
    "for video, group in grouped:\n",
    "    seq = group[feature_cols].values\n",
    "    X_sequences.append(seq)\n",
    "    y_action_seq.append(group['action'].iloc[0])\n",
    "\n",
    "# Dividir en conjuntos de entrenamiento y prueba\n",
    "indices = np.arange(len(X_sequences))\n",
    "train_idx, test_idx = train_test_split(indices, test_size=0.3, random_state=42)\n",
    "\n",
    "X_train_seq = [X_sequences[i] for i in train_idx]\n",
    "y_train_seq = [y_action_seq[i] for i in train_idx]\n",
    "X_test_seq  = [X_sequences[i] for i in test_idx]\n",
    "y_test_seq  = [y_action_seq[i] for i in test_idx]\n",
    "\n",
    "# Funcion para calcular la distancia DTW entre dos secuencias usando distancia\n",
    "def dtw_distance(seq1, seq2):\n",
    "    distance, path = fastdtw(seq1, seq2, dist=euclidean)\n",
    "    return distance\n",
    "\n",
    "# Funcion para predecir la etiqueta de acción mediante kNN (con DTW)\n",
    "def predict_knn_dtw(test_seq, train_sequences, train_labels, k=1):\n",
    "    distances = []\n",
    "    for seq, label in zip(train_sequences, train_labels):\n",
    "        d = dtw_distance(test_seq, seq)\n",
    "        distances.append((d, label))\n",
    "    distances.sort(key=lambda x: x[0])\n",
    "    # Para k=1, se retorna la etiqueta del vecino más cercano.\n",
    "    return distances[0][1]\n",
    "\n",
    "# Predecir en el conjunto de prueba usando DTW kNN\n",
    "y_pred_dtw = [predict_knn_dtw(seq, X_train_seq, y_train_seq, k=1) for seq in X_test_seq]\n",
    "print(\"=== Clasificación de Acción con DTW kNN ===\")\n",
    "print(\"Precisión:\", accuracy_score(y_test_seq, y_pred_dtw))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "26c01abc-2813-41d7-9c7a-643c0aea4bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Clasificacion de Acción con RandomForest ===\n",
      "Precisión: 0.9777777777777777\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Action 1       1.00      1.00      1.00        16\n",
      "    Action 2       1.00      1.00      1.00        16\n",
      "    Action 3       0.91      1.00      0.95        21\n",
      "    Action 4       1.00      0.89      0.94        18\n",
      "    Action 5       1.00      1.00      1.00        19\n",
      "\n",
      "    accuracy                           0.98        90\n",
      "   macro avg       0.98      0.98      0.98        90\n",
      "weighted avg       0.98      0.98      0.98        90\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cargar y preparar datos desde \"pose_data.csv\"\n",
    "df = pd.read_csv(\"pose_data.csv\")\n",
    "feature_cols = [col for col in df.columns if col.startswith(\"feat\")]\n",
    "\n",
    "# Ordenar por video y frame\n",
    "df = df.sort_values(['video', 'frame'])\n",
    "\n",
    "# Agrupar frames por video\n",
    "grouped = df.groupby('video')\n",
    "\n",
    "# Funcion para extraer caracteristicas de una secuencia\n",
    "def extract_features_from_sequence(seq):\n",
    "    mean_features = np.mean(seq, axis=0)\n",
    "    std_features = np.std(seq, axis=0)\n",
    "    return np.concatenate([mean_features, std_features])\n",
    "\n",
    "X_features = []\n",
    "y_action = []\n",
    "y_completion = []\n",
    "\n",
    "for video, group in grouped:\n",
    "    seq = group[feature_cols].values\n",
    "    features = extract_features_from_sequence(seq)\n",
    "    X_features.append(features)\n",
    "    \n",
    "    y_action.append(group['action'].iloc[0])\n",
    "    y_completion.append(group['completion'].iloc[0])\n",
    "\n",
    "X_features = np.array(X_features)\n",
    "y_action = np.array(y_action)\n",
    "y_completion = np.array(y_completion)\n",
    "\n",
    "# Codificar las etiquetas de accion para la clasificacion\n",
    "le = LabelEncoder()\n",
    "y_action_enc = le.fit_transform(y_action)\n",
    "\n",
    "# Dividir los datos por toma (videos)\n",
    "X_train, X_test, y_train_action, y_test_action, y_train_completion, y_test_completion = train_test_split(\n",
    "    X_features, y_action_enc, y_completion, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train_action)\n",
    "y_pred_action = clf.predict(X_test)\n",
    "\n",
    "print(\"\\n=== Clasificacion de Acción con RandomForest ===\")\n",
    "print(\"Precisión:\", accuracy_score(y_test_action, y_pred_action))\n",
    "print(classification_report(y_test_action, y_pred_action, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32e1007-4771-42e4-ba56-84ec3c59fabc",
   "metadata": {},
   "source": [
    "======== Para predecir completion =========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eb53776b-082c-4650-ba07-a356a0010cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Regresión de Finalización con RandomForest ===\n",
      "MSE: 0.05085194444444444\n",
      "MAE: 0.16727777777777778\n",
      "Precision: 75.55555555555556 %\n",
      "\n",
      "Matriz de Confusion:\n",
      "               Predicho 0.0  Predicho 0.5  Predicho 1.0\n",
      "Verdadero 0.0            23             5             2\n",
      "Verdadero 0.5             3            20             2\n",
      "Verdadero 1.0             0            10            25\n"
     ]
    }
   ],
   "source": [
    "# Entrenar un regresor RandomForest para la ver el completion\n",
    "reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "reg.fit(X_train, y_train_completion)\n",
    "y_pred_completion = reg.predict(X_test)\n",
    "\n",
    "# Definir las categorías validas de las etiquetas\n",
    "completion_mapping = {0.0: 0, 0.5: 1, 1.0: 2}\n",
    "\n",
    "# Funcion para redondear las predicciones al valor de finalización más cercano (0.0, 0.5 o 1.0)\n",
    "def round_to_closest_completion(value):\n",
    "    return min([0.0, 0.5, 1.0], key=lambda x: abs(x - value))\n",
    "\n",
    "y_pred_completion_rounded = np.array([round_to_closest_completion(y) for y in y_pred_completion])\n",
    "\n",
    "# Convertir los valores de finalizacion a etiquetas discretas\n",
    "y_test_completion_class = np.array([completion_mapping[val] for val in y_test_completion])\n",
    "y_pred_completion_rounded_class = np.array([completion_mapping[val] for val in y_pred_completion_rounded])\n",
    "\n",
    "# Calcular la precisión de las predicciones redondeadas tratandolas como clases\n",
    "accuracy_completion = np.mean(y_test_completion_class == y_pred_completion_rounded_class)\n",
    "\n",
    "# Calcular el mae\n",
    "mae_completion = np.mean(np.abs(y_test_completion - y_pred_completion))\n",
    "\n",
    "print(\"=== Regresión de Finalización con RandomForest ===\")\n",
    "print(\"MSE:\", mean_squared_error(y_test_completion, y_pred_completion))\n",
    "print(\"MAE:\", mae_completion)\n",
    "print(\"Precision:\", accuracy_completion * 100, \"%\")\n",
    "\n",
    "# Calcular y mostrar la matriz de confusión\n",
    "conf_matrix = confusion_matrix(y_test_completion_class, y_pred_completion_rounded_class, labels=[0, 1, 2])\n",
    "print(\"\\nMatriz de Confusion:\")\n",
    "print(pd.DataFrame(conf_matrix, index=[\"Verdadero 0.0\", \"Verdadero 0.5\", \"Verdadero 1.0\"], \n",
    "                   columns=[\"Predicho 0.0\", \"Predicho 0.5\", \"Predicho 1.0\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6aecd6-721d-42f8-8410-7bb160772c26",
   "metadata": {},
   "source": [
    "==================== Pruebas ===================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ad9ed5d8-a3f3-4e09-8f17-9717aa736f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = get_videos(\"Test_Videos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a9989596-511e-46a9-89aa-42a89e4d8e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pose data saved to pose_data_test.csv\n"
     ]
    }
   ],
   "source": [
    "data_to_csv(output_data,\"pose_data_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5c1b63a2-32c2-48fc-9285-0983a8cd7467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DTW kNN ===\n",
      "En el video: Person 1 Action 1 take 1.mp4 -> Se predijo: Action 1\n",
      "En el video: Person 1 Action 1 take 1.mp4_aug -> Se predijo: Action 1\n",
      "En el video: Person 2 Action 1 take 2.mp4 -> Se predijo: Action 2\n",
      "En el video: Person 2 Action 1 take 2.mp4_aug -> Se predijo: Action 2\n",
      "\n",
      "Precision: 0.5\n"
     ]
    }
   ],
   "source": [
    "df_test = pd.read_csv(\"pose_data_test.csv\")\n",
    "df_test = df_test.sort_values(['video', 'frame'])\n",
    "\n",
    "X_test_new_seq = []\n",
    "y_test_new_seq = []  \n",
    "test_videos = df_test[\"video\"].unique()\n",
    "\n",
    "for video in test_videos:\n",
    "    group = df_test[df_test[\"video\"] == video]\n",
    "    seq = group[feature_cols].values\n",
    "    X_test_new_seq.append(seq)\n",
    "    \n",
    "\n",
    "    if \"action\" in df_test.columns:\n",
    "        y_test_new_seq.append(group['action'].iloc[0])\n",
    "\n",
    "y_pred_new_dtw = [predict_knn_dtw(seq, X_train_seq, y_train_seq, k=1) for seq in X_test_new_seq]\n",
    "\n",
    "print(\"=== DTW kNN ===\")\n",
    "for video_name, predicted_action in zip(test_videos, y_pred_new_dtw):\n",
    "    print(f\"En el video: {video_name} -> Se predijo: {predicted_action}\")\n",
    "\n",
    "if len(y_test_new_seq) > 0:\n",
    "    accuracy = accuracy_score(y_test_new_seq, y_pred_new_dtw)\n",
    "    print(\"\\nPrecision:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "41c3b126-d900-4429-b5b9-8893316f4c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Random Forest Predictions on pose_data_test.csv ===\n",
      "Video: Person 1 Action 1 take 1.mp4 -> Predicted Action: Action 1, Predicted Completion: 1.0\n",
      "Video: Person 1 Action 1 take 1.mp4_aug -> Predicted Action: Action 1, Predicted Completion: 1.0\n",
      "Video: Person 2 Action 1 take 2.mp4 -> Predicted Action: Action 2, Predicted Completion: 0.5\n",
      "Video: Person 2 Action 1 take 2.mp4_aug -> Predicted Action: Action 2, Predicted Completion: 0.5\n",
      "\n",
      "Action Classification Accuracy on pose_data_test.csv: 0.5\n",
      "\n",
      "Completion Prediction Accuracy on pose_data_test.csv: 1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_test = pd.read_csv(\"pose_data_test.csv\")\n",
    "df_test = df_test.sort_values(['video', 'frame'])\n",
    "\n",
    "X_test_new_features = []\n",
    "y_test_new_action = []\n",
    "y_test_new_completion = []\n",
    "test_videos = df_test[\"video\"].unique()\n",
    "\n",
    "for video in test_videos:\n",
    "    group = df_test[df_test[\"video\"] == video]\n",
    "    seq = group[feature_cols].values\n",
    "    features = extract_features_from_sequence(seq)\n",
    "    X_test_new_features.append(features)\n",
    "    \n",
    "    if \"action\" in df_test.columns:\n",
    "        y_test_new_action.append(group['action'].iloc[0])\n",
    "    if \"completion\" in df_test.columns:\n",
    "        y_test_new_completion.append(group['completion'].iloc[0])\n",
    "\n",
    "X_test_new_features = np.array(X_test_new_features)\n",
    "\n",
    "y_pred_new_action = clf.predict(X_test_new_features)\n",
    "y_pred_new_completion = reg.predict(X_test_new_features)\n",
    "\n",
    "y_pred_new_completion_rounded = np.array([round_to_closest_completion(y) for y in y_pred_new_completion])\n",
    "\n",
    "print(\"=== Random Forest Predictions on pose_data_test.csv ===\")\n",
    "for video_name, predicted_action, predicted_completion in zip(test_videos, y_pred_new_action, y_pred_new_completion_rounded):\n",
    "    predicted_action_label = le.inverse_transform([predicted_action])[0]\n",
    "    print(f\"Video: {video_name} -> Predicted Action: {predicted_action_label}, Predicted Completion: {predicted_completion}\")\n",
    "\n",
    "if len(y_test_new_action) > 0:\n",
    "    y_test_new_action_enc = le.transform(y_test_new_action)\n",
    "    accuracy_action = accuracy_score(y_test_new_action_enc, y_pred_new_action)\n",
    "    print(\"\\nAction Classification Accuracy on pose_data_test.csv:\", accuracy_action)\n",
    "\n",
    "if len(y_test_new_completion) > 0:\n",
    "    y_test_new_completion_class = np.array([completion_mapping[val] for val in y_test_new_completion])\n",
    "    y_pred_new_completion_class = np.array([completion_mapping[val] for val in y_pred_new_completion_rounded])\n",
    "\n",
    "    accuracy_completion = np.mean(y_test_new_completion_class == y_pred_new_completion_class)\n",
    "    print(\"\\nCompletion Prediction Accuracy on pose_data_test.csv:\", accuracy_completion)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
